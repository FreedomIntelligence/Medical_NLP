# Chinese_medical_NLP

>ğŸš€ğŸš€ğŸš€æ¬¢è¿å¤§å®¶å‚ä¸å…±å»ºï¼š
>
>ç”±äºæœ¬äºº2021å¹´ç¦»å¼€åŒ»ç–—NLPé¢†åŸŸï¼Œè¿™ä¸ªrepoå·²ç»è¾ƒé•¿æ—¶é—´æ²¡æœ‰æ›´æ–°ï¼Œå¦‚æœæœ‰ä»äº‹ç›¸å…³é¢†åŸŸçš„åŒå­¦æƒ³è¦ç»§ç»­ç»´æŠ¤æ­¤repoï¼Œå¯ä»¥è”ç³»æˆ‘æŠŠä½ æ·»åŠ ä¸ºå…±å»ºè€…ï¼ˆæˆ–å…¶ä»–æ–¹å¼ï¼‰ã€‚
>
>ğŸ¤ğŸ¤ğŸ¤
>
>wx: 18810877926  å¤‡æ³¨ï¼š github åŒ»ç–—




åŒ»ç–—NLPé¢†åŸŸï¼ˆä¸»è¦å…³æ³¨ä¸­æ–‡ï¼‰   è¯„æµ‹æ•°æ®é›† ä¸ è®ºæ–‡ç­‰ç›¸å…³èµ„æºã€‚

   * [Chinese_medical_NLP](#chinese_medical_nlp)
     * [è¯„æµ‹/æ¯”èµ›æ›´æ–°](#è¯„æµ‹åŠæ¯”èµ›æ›´æ–°)
       * [1. MEDIQA 2021](#mediqa-2021)
       * [2. ICLR 2021 åŒ»ç–—å¯¹è¯ç”Ÿæˆä¸è‡ªåŠ¨è¯Šæ–­å›½é™…ç«èµ›](#iclr-2021-åŒ»ç–—å¯¹è¯ç”Ÿæˆä¸è‡ªåŠ¨è¯Šæ–­å›½é™…ç«èµ›)
       * [3. ä¸­æ–‡åŒ»ç–—ä¿¡æ¯å¤„ç†æŒ‘æˆ˜æ¦œCBLUEæ•°æ®é›†](#ä¸­æ–‡åŒ»ç–—ä¿¡æ¯å¤„ç†æŒ‘æˆ˜æ¦œCBLUEæ•°æ®é›†)
       * [4. ä¸­å›½è®¡ç®—è¯­è¨€å­¦å¤§ä¼šCCL 2021æ™ºèƒ½åŒ»ç–—å¯¹è¯è¯Šç–—è¯„æµ‹ä»»åŠ¡](#ä¸­å›½è®¡ç®—è¯­è¨€å­¦å¤§ä¼šCCL-2021æ™ºèƒ½åŒ»ç–—å¯¹è¯è¯Šç–—è¯„æµ‹ä»»åŠ¡)
       * [5. ç§‘å¤§è®¯é£ åŒ»ç–—å®ä½“åŠå…³ç³»è¯†åˆ«æŒ‘æˆ˜èµ›](#ç§‘å¤§è®¯é£-åŒ»ç–—å®ä½“åŠå…³ç³»è¯†åˆ«æŒ‘æˆ˜èµ›)
     * [ä¸­æ–‡æ•°æ®é›†](#ä¸­æ–‡æ•°æ®é›†)
       * [1. Yidu-S4Kï¼šåŒ»æ¸¡äº‘ç»“æ„åŒ–4Kæ•°æ®é›†](#1-yidu-s4kåŒ»æ¸¡äº‘ç»“æ„åŒ–4kæ•°æ®é›†)
       * [2.ç‘é‡‘åŒ»é™¢ç³–å°¿ç—…æ•°æ®é›†](#2ç‘é‡‘åŒ»é™¢ç³–å°¿ç—…æ•°æ®é›†)
       * [3.Yidu-N7Kï¼šåŒ»æ¸¡äº‘æ ‡å‡†åŒ–7Kæ•°æ®é›†](#3yidu-n7kåŒ»æ¸¡äº‘æ ‡å‡†åŒ–7kæ•°æ®é›†)
       * [4.ä¸­æ–‡åŒ»å­¦é—®ç­”æ•°æ®é›†](#4ä¸­æ–‡åŒ»å­¦é—®ç­”æ•°æ®é›†)
       * [5.å¹³å®‰åŒ»ç–—ç§‘æŠ€ç–¾ç—…é—®ç­”è¿ç§»å­¦ä¹ æ¯”èµ›](#5å¹³å®‰åŒ»ç–—ç§‘æŠ€ç–¾ç—…é—®ç­”è¿ç§»å­¦ä¹ æ¯”èµ›)
       * [6.å¤©æ± æ–°å† è‚ºç‚é—®å¥åŒ¹é…æ¯”èµ›](#6å¤©æ± æ–°å† è‚ºç‚é—®å¥åŒ¹é…æ¯”èµ›)
       * [7.ä¸­æ–‡åŒ»æ‚£é—®ç­”å¯¹è¯æ•°æ®](#7ä¸­æ–‡åŒ»æ‚£é—®ç­”å¯¹è¯æ•°æ®)
       * [8.ä¸­æ–‡åŒ»å­¦é—®ç­”æ•°æ®](#8ä¸­æ–‡åŒ»å­¦é—®ç­”æ•°æ®)
       * [9.CHIP2020å„é¡¹è¯„æµ‹å·²å¼€æ”¾](#9chip2020å„é¡¹è¯„æµ‹å·²å¼€æ”¾)
       * [10.åŒ»å­¦æ•°æ®æŒ–æ˜ä¸ç®—æ³•è¯„æµ‹å¤§èµ›](#10åŒ»å­¦æ•°æ®æŒ–æ˜ä¸ç®—æ³•è¯„æµ‹å¤§èµ›)
       * [11.ä¸­æ–‡åŒ»ç–—å¯¹è¯æ•°æ®é›†](#11ä¸­æ–‡åŒ»ç–—å¯¹è¯æ•°æ®é›†)
       * [12.é˜¿é‡Œå‘å¸ƒçš„ä¸­æ–‡åŒ»ç–—æ ‡å‡†æ•°æ®é›†åˆ](#12é˜¿é‡Œå‘å¸ƒçš„ä¸­æ–‡åŒ»ç–—æ ‡å‡†æ•°æ®é›†åˆ)
     * [ä¸­æ–‡åŒ»å­¦çŸ¥è¯†å›¾è°±](#ä¸­æ–‡åŒ»å­¦çŸ¥è¯†å›¾è°±)
       * [CMeKG](#cmekg)
     * [è‹±æ–‡æ•°æ®é›†](#è‹±æ–‡æ•°æ®é›†)
       * [PubMedQA: A Dataset for Biomedical Research Question Answering](#pubmedqa-a-dataset-for-biomedical-research-question-answering)
       * [COMETA: A Corpus for Medical Entity Linking in the Social Media](#cometa-a-corpus-for-medical-entity-linking-in-the-social-media)
       * [MedMentions](#medmentions)
     * [ç›¸å…³è®ºæ–‡](#ç›¸å…³è®ºæ–‡)
       * [1.åŒ»ç–—é¢†åŸŸé¢„è®­ç»ƒembedding](#1åŒ»ç–—é¢†åŸŸé¢„è®­ç»ƒembedding)
       * [2.ç»¼è¿°ç±»æ–‡ç« ](#2ç»¼è¿°ç±»æ–‡ç« )
       * [3.ç”µå­ç—…å†ç›¸å…³æ–‡ç« ](#3ç”µå­ç—…å†ç›¸å…³æ–‡ç« )
       * [4.åŒ»å­¦å…³ç³»æŠ½å–](#4åŒ»å­¦å…³ç³»æŠ½å–)
       * [5.åŒ»å­¦çŸ¥è¯†å›¾è°±](#5åŒ»å­¦çŸ¥è¯†å›¾è°±)
       * [6.è¾…åŠ©è¯Šæ–­](#6è¾…åŠ©è¯Šæ–­)
       * [7.ACL2020åŒ»å­¦é¢†åŸŸç›¸å…³è®ºæ–‡åˆ—è¡¨](#7acl2020åŒ»å­¦é¢†åŸŸç›¸å…³è®ºæ–‡åˆ—è¡¨)
       * [8.åŒ»ç–—å®ä½“Linkingï¼ˆæ ‡å‡†åŒ–ï¼‰](#8åŒ»ç–—å®ä½“linkingæ ‡å‡†åŒ–)
       * [9. AAAI2020 åŒ»å­¦NLPç›¸å…³è®ºæ–‡åˆ—è¡¨](#9-aaai2020-åŒ»å­¦nlpç›¸å…³è®ºæ–‡åˆ—è¡¨)
       * [10. EMNLP2020 åŒ»å­¦NLPç›¸å…³è®ºæ–‡åˆ—è¡¨](#10-emnlp2020-åŒ»å­¦nlpç›¸å…³è®ºæ–‡åˆ—è¡¨)
     * [ä¸­æ–‡åŒ»ç–—é¢†åŸŸè¯­æ–™](#ä¸­æ–‡åŒ»ç–—é¢†åŸŸè¯­æ–™)
       * [åŒ»å­¦æ•™æ åŸ¹è®­è€ƒè¯•](#åŒ»å­¦æ•™æåŸ¹è®­è€ƒè¯•)
       * [å“ˆå·¥å¤§ã€Šå¤§è¯æ—ã€‹å¼€æ”¾75ä¸‡æ ¸å¿ƒå®ä½“è¯åŠç›¸å…³æ¦‚å¿µã€å…³ç³»åˆ—è¡¨ï¼ˆåŒ…å«ä¸­è¯/åŒ»é™¢/ç”Ÿç‰© ç±»åˆ«ï¼‰](#å“ˆå·¥å¤§å¤§è¯æ—å¼€æ”¾75ä¸‡æ ¸å¿ƒå®ä½“è¯åŠç›¸å…³æ¦‚å¿µå…³ç³»åˆ—è¡¨åŒ…å«ä¸­è¯åŒ»é™¢ç”Ÿç‰©-ç±»åˆ«)
     * [åŒ»å­¦embeddingåŠé¢„è®­ç»ƒæ¨¡å‹](#åŒ»å­¦embeddingåŠé¢„è®­ç»ƒæ¨¡å‹)
       * [å¼€æºè‹±æ–‡åŒ»å­¦embedding](#å¼€æºè‹±æ–‡åŒ»å­¦embedding)
       * [MC-BERTä¸­æ–‡åŒ»ç–—é¢„è®­ç»ƒæ¨¡å‹](#MC-BERTä¸­æ–‡åŒ»ç–—é¢„è®­ç»ƒæ¨¡å‹)
     * [å¼€æºå·¥å…·åŒ…](#å¼€æºå·¥å…·åŒ…)
       * [åˆ†è¯å·¥å…·](#åˆ†è¯å·¥å…·)
         * [PKUSEG](#pkuseg)
     * [å·¥ä¸šçº§äº§å“è§£å†³æ–¹æ¡ˆ](#å·¥ä¸šçº§äº§å“è§£å†³æ–¹æ¡ˆ)
     * [blogåˆ†äº«](#blogåˆ†äº«)
     * [å‹æƒ…é“¾æ¥](#å‹æƒ…é“¾æ¥)

## è¯„æµ‹åŠæ¯”èµ›æ›´æ–°

è¯´æ˜: è¿™éƒ¨åˆ†ä»2021.2.20å¼€å§‹æ›´æ–°ã€‚æ·»åŠ å½“æ—¶è¿˜æœªæˆªæ­¢çš„ä¸­è‹±æ–‡åŒ»ç–—ç›¸å…³çš„NLPè¯„æµ‹æˆ–æ¯”èµ›ã€‚

### MEDIQA-2021

æ¥æºï¼š NAACL 2021 workshop

Introduction

MEDIQA 2021 tackles three summarization tasks in the medical domain: consumer health question summarization, multi-answer summarization, and radiology report summarization. In this shared task, we will also explore the use of different evaluation metrics for summarization.

MEDIQA 2021 will be organized at the NAACL-BioNLP 2021 workshop.

[BioNLP Workshop](https://aclweb.org/aclwiki/BioNLP_Workshop)

[MEDIQAè¯„æµ‹åœ°å€](https://sites.google.com/view/mediqa2021)

### ICLR-2021-åŒ»ç–—å¯¹è¯ç”Ÿæˆä¸è‡ªåŠ¨è¯Šæ–­å›½é™…ç«èµ›

æ¥æº: ICLR 2021 workshop

æœ¬æ¬¡ç«èµ›ä»¥è‡ªåŠ¨åŒ»ç–—è¯Šæ–­å¯¹è¯ç³»ç»Ÿçš„å¼€å‘ä¸ºä¸»é¢˜ï¼Œç›®å‰è®¾ç½®äº†ä¸¤å¤§èµ›é“ï¼šåŒ»ç–—å¯¹è¯ç”Ÿæˆèµ›é“å’Œè‡ªåŠ¨åŒ»ç–—è¯Šæ–­èµ›é“ã€‚

[ç«èµ›å®˜æ–¹åœ°å€](https://mlpcp21.github.io/pages/challenge)

### ä¸­æ–‡åŒ»ç–—ä¿¡æ¯å¤„ç†æŒ‘æˆ˜æ¦œCBLUEæ•°æ®é›†

è¯„æµ‹ä»‹ç»:

ä¸­æ–‡åŒ»ç–—ä¿¡æ¯å¤„ç†æŒ‘æˆ˜æ¦œCBLUE(Chinese Biomedical Language Understanding Evaluation)æ˜¯ä¸­å›½ä¸­æ–‡ä¿¡æ¯å­¦ä¼šåŒ»ç–—å¥åº·ä¸ç”Ÿç‰©ä¿¡æ¯å¤„ç†ä¸“ä¸šå§”å‘˜ä¼šåœ¨åˆæ³•å¼€æ”¾å…±äº«çš„ç†å¿µä¸‹å‘èµ·ï¼Œç”±é˜¿é‡Œäº‘å¤©æ± å¹³å°æ‰¿åŠï¼Œå¹¶ç”±åŒ»æ¸¡äº‘ï¼ˆåŒ—äº¬ï¼‰æŠ€æœ¯æœ‰é™å…¬å¸ã€å¹³å®‰åŒ»ç–—ç§‘æŠ€ã€åŒ—äº¬å¤§å­¦ã€éƒ‘å·å¤§å­¦ã€é¹åŸå®éªŒå®¤ã€å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦(æ·±åœ³ï¼‰ã€åŒæµå¤§å­¦ã€å¤¸å…‹ã€é˜¿é‡Œå·´å·´è¾¾æ‘©é™¢ç­‰å¼€å±•æ™ºæ…§åŒ»ç–—ç ”ç©¶çš„å•ä½å…±åŒååŠï¼Œæ—¨åœ¨æ¨åŠ¨ä¸­æ–‡åŒ»å­¦NLPæŠ€æœ¯å’Œç¤¾åŒºçš„å‘å±•ã€‚è¯„æµ‹é•¿æœŸå¼€æ”¾ã€‚

CBLUE 1.0æ˜¯ç”±CHIPä¼šè®®å¾€å±Šçš„å­¦æœ¯è¯„æµ‹æ¯”èµ›å’Œé˜¿é‡Œå¤¸å…‹åŒ»ç–—æœç´¢ä¸šåŠ¡çš„æ•°æ®é›†ç»„æˆï¼ŒåŒ…æ‹¬åŒ»å­¦æ–‡æœ¬ä¿¡æ¯æŠ½å–ï¼ˆå®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–ï¼‰ã€åŒ»å­¦æœ¯è¯­å½’ä¸€åŒ–ã€åŒ»å­¦æ–‡æœ¬åˆ†ç±»ã€åŒ»å­¦å¥å­å…³ç³»åˆ¤å®šå’ŒåŒ»å­¦QAå…±5å¤§ç±»ä»»åŠ¡8ä¸ªå­ä»»åŠ¡ã€‚

[CBLUEè¯„æµ‹å®˜æ–¹åœ°å€](https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414)

[è®ºæ–‡åœ°å€](https://arxiv.org/abs/2106.08087)

[Githubé¡¹ç›®åœ°å€](https://github.com/CBLUEbenchmark/CBLUE)


### ä¸­å›½è®¡ç®—è¯­è¨€å­¦å¤§ä¼šCCL-2021æ™ºèƒ½åŒ»ç–—å¯¹è¯è¯Šç–—è¯„æµ‹ä»»åŠ¡

ä¸­å›½è®¡ç®—è¯­è¨€å­¦å¤§ä¼šï¼ˆCCL 2021ï¼‰å‘å¸ƒ5é¡¹æŠ€æœ¯è¯„æµ‹ä»»åŠ¡ï¼Œå…¶ä¸­åŒ…å«â€œæ™ºèƒ½åŒ»ç–—å¯¹è¯è¯Šç–—â€ä»»åŠ¡ã€‚

ä»»åŠ¡ä»‹ç»:

æœ¬æ¬¡æ™ºèƒ½å¯¹è¯è¯Šç–—è¯„æµ‹è®¾ç½®3ä¸ªèµ›é“ã€‚

èµ›é“ä¸€ï¼šåŒ»æ‚£å¯¹è¯ç†è§£
åŒ»æ‚£å¯¹è¯ç†è§£æ—¨åœ¨å¯¹é—®è¯Šæ–‡æœ¬ä¿¡æ¯è¿›è¡Œä¿¡æ¯æŠ½å–ï¼Œä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªä»»åŠ¡ï¼Œåˆ†åˆ«æ˜¯å‘½åå®ä½“è¯†åˆ«å’Œç—‡çŠ¶æ£€æŸ¥è¯†åˆ«ã€‚

ä»»åŠ¡1ï¼šå‘½åå®ä½“è¯†åˆ«ã€‚ä»åŒ»æ‚£å¯¹è¯æ–‡æœ¬ä¸­è¯†åˆ«å‡ºäº”ç±»é‡è¦çš„åŒ»ç–—ç›¸å…³å®ä½“ã€‚
ä»»åŠ¡2ï¼šç—‡çŠ¶è¯†åˆ«ã€‚æ ¹æ®åŒ»æ‚£å¯¹è¯æ–‡æœ¬ï¼Œè¯†åˆ«å‡ºç—…äººå…·æœ‰çš„ç—‡çŠ¶ä¿¡æ¯ã€‚
æœ¬èµ›é“æ•°æ®é›†åŒ…æ‹¬è¶…è¿‡3000ç»„åŒ»æ‚£å¯¹è¯æ¡ˆä¾‹æ ·æœ¬ï¼Œè¦†ç›–6ç§å„¿ç§‘ç–¾ç—…ï¼Œ10ä¸‡ä½™å¥å¯¹è¯ï¼Œæ ·æœ¬å¹³å‡å¯¹è¯æ¬¡æ•°ä¸º40æ¬¡ï¼Œ å¹³å‡æ¯ä¸ªæ ·æœ¬çš„å¯¹è¯å­—æ•°ä¸º523ä¸ªã€‚

èµ›é“äºŒï¼šåŒ»ç–—æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆ
åŒ»ç–—æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆæ—¨åœ¨å¯¹é—®è¯Šè¿‡ç¨‹ä¸­è¿›è¡Œä¿¡æ¯çš„æ€»ç»“ï¼Œä»»åŠ¡è¦æ±‚å‚èµ›å›¢é˜Ÿèƒ½ä¾æ®ç—…äººè‡ªè¿°å’ŒåŒ»æ‚£å¯¹è¯ï¼Œ è¾“å‡ºå…·æœ‰è§„å®šæ ¼å¼çš„åŒ»ç–—æŠ¥å‘Šã€‚æŠ¥å‘Šéœ€è¦åŒ…å«6ä¸ªéƒ¨åˆ†ï¼šä¸»è¯‰ã€ç°ç—…å²ã€è¾…åŠ©æ£€æŸ¥ã€æ—¢å¾€å²ã€è¯Šæ–­å’Œå»ºè®®ã€‚

ä»»åŠ¡1ï¼šåŒ»ç–—æŠ¥å‘Šç”Ÿæˆã€‚ä¾æ®ç—…äººè‡ªè¿°å’ŒåŒ»æ‚£å¯¹è¯ï¼Œè¾“å‡ºå…·æœ‰è§„å®šæ ¼å¼çš„åŒ»ç–—æŠ¥å‘Šã€‚
æœ¬èµ›é“æ•°æ®é›†åŒ…æ‹¬è¶…è¿‡3000ç»„åŒ»æ‚£å¯¹è¯æ¡ˆä¾‹æ ·æœ¬ï¼Œè¦†ç›–6ç§å„¿ç§‘ç–¾ç—…ï¼Œ10ä¸‡ä½™å¥å¯¹è¯ï¼Œæ ·æœ¬å¹³å‡å¯¹è¯æ¬¡æ•°ä¸º40æ¬¡ï¼Œ å¹³å‡æ¯ä¸ªæ ·æœ¬çš„å¯¹è¯å­—æ•°ä¸º523ä¸ªã€‚

èµ›é“ä¸‰ï¼šæ™ºèƒ½åŒ–åŒ»ç–—è¯Šæ–­
å°±è¯Šè¿‡ç¨‹æ˜¯ä¸€ä¸ªå¸¦æœ‰ç›®çš„çš„åºåˆ—åŒ–åŒ»ç”Ÿ-æ‚£è€…äº¤äº’çš„è¿‡ç¨‹ã€‚æ™ºèƒ½åŒ–åŒ»ç–—è¯Šæ–­æ˜¯ä»»åŠ¡å‹å¯¹è¯ç³»ç»Ÿçš„é‡ç‚¹ç ”ç©¶æ–¹å‘ã€‚

ä»»åŠ¡1ï¼šé¢å‘è‡ªåŠ¨è¯Šç–—çš„å¯¹è¯ç³»ç»Ÿã€‚è¦æ±‚å‚èµ›ç³»ç»Ÿæ ¹æ®ç»™å‡ºçš„æ˜¾æ€§ä¿¡æ¯ ï¼ˆç—…äººè‡ªè¯‰ä¸­æåŠçš„ç—‡çŠ¶ã€æ£€æŸ¥ï¼‰ï¼Œä¸ç—…äººæ¨¡æ‹Ÿå™¨è¿›è¡Œäº’åŠ¨ä»¥è·å–æ›´å¤šç—…äººçš„ç—‡çŠ¶ã€å·²åšçš„åŒ»ç–—æ£€æŸ¥ï¼Œ ä¾æ®äº¤äº’å†…å®¹åˆ¤æ–­ç–¾ç—…ï¼Œå¹¶ç»™å‡ºç›¸åº”çš„æ£€æŸ¥å»ºè®®ã€‚
æœ¬èµ›é“çš„æ•°æ®é›†è¶…è¿‡2000ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ç–¾ç—…ç±»åˆ«ã€ç—…äººè‡ªè¯‰æ–‡æœ¬ã€ç›´æ¥ä¿¡æ¯ ï¼ˆç—…äººè‡ªè¯‰ä¸­æ˜ç¡®æåŠçš„å®ä½“ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç—‡çŠ¶å’Œæ£€æŸ¥ï¼‰ã€éšè—ä¿¡æ¯ï¼ˆç»“åˆæ•´æ®µåŒ»æ‚£å¯¹è¯å¾—åˆ°çš„å®ä½“åŠæ ‡ç­¾ï¼Œè¡¨ç¤ºæ‚£è€…æ˜¯å¦å·²ç»æœ‰è¯¥ç—‡çŠ¶ã€æ˜¯å¦å·²ç»åšè¿‡è¯¥æ£€æŸ¥ï¼‰ã€‚

[ä»»åŠ¡ç½‘å€](http://www.fudan-disc.com/sharedtask/imcs21/index.html)

é™„ï¼š

[CCL2021è¯„æµ‹å®˜ç½‘](http://cips-cl.org/static/CCL2021/cclEval/taskEvaluation/index.html)

### ç§‘å¤§è®¯é£-åŒ»ç–—å®ä½“åŠå…³ç³»è¯†åˆ«æŒ‘æˆ˜èµ›

è¯„æµ‹ä»‹ç»ï¼šç”µå­ç—…å†æ˜¯åŒ»ç–—æœºæ„å¯¹é—¨è¯Šã€ä½é™¢æ‚£è€…è¿›è¡Œä¸´åºŠæ²»ç–—å’ŒæŒ‡å¯¼å¹²é¢„çš„æ•°å­—åŒ–åŒ»ç–—æœåŠ¡å·¥ä½œè®°å½•ï¼ŒåŒ…å«äº†å¤§é‡çš„æ‚£è€…åŒ»å­¦ä¿¡æ¯ã€‚åŒ»æŠ€æŠ¥å‘Šå•æ˜¯ç”µå­ç—…å†ååˆ†é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…å«äº†æ‚£è€…è¯¦ç»†çš„æ£€æŸ¥æ£€éªŒä¿¡æ¯ï¼Œå¦‚è¶…å£°ã€CTå’Œç£å…±æŒ¯ç­‰ã€‚ä½†æ˜¯ï¼ŒåŒ»æŠ€æŠ¥å‘Šå•æ˜¯ä¸€ç§åŠç»“æ„åŒ–çš„æ•°æ®ï¼Œä¸åŒåŒ»ç”Ÿçš„è¡¨è¿°é£æ ¼ä¸ä¸€è‡´ï¼Œæ–‡æœ¬å½¢å¼ç¼ºä¹ç»Ÿä¸€çš„è§„èŒƒï¼Œå› æ­¤å°†å…¶ä¸­éç»“æ„åŒ–çš„éƒ¨åˆ†è½¬æ¢ä¸ºç»“æ„åŒ–çš„ä¿¡æ¯æ˜¯éå¸¸é‡è¦çš„ï¼Œå¯ä»¥æœ‰æ•ˆçš„æé«˜åŒ»ç”Ÿå·¥ä½œæ•ˆç‡ï¼Œä¼˜åŒ–åŒ»ç–—æœºæ„æµç¨‹ã€‚åŒ»æŠ€é¢†åŸŸçš„å‘½åå®ä½“å’Œå®ä½“å…³ç³»è¯†åˆ«æ˜¯ç›®å‰çš„ç ”ç©¶çƒ­ç‚¹ä¹‹ä¸€ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ç”µå­ç—…å†ä¿¡æ¯æŠ½å–çš„é‡è¦ç ”ç©¶å†…å®¹ã€‚è¯„æµ‹ä»»åŠ¡åŒ…å«åŒ»å­¦å®ä½“è¯†åˆ«+åŒ»å­¦å…³ç³»æŠ½å–ã€‚

[è¯„æµ‹å®˜ç½‘](https://challenge.xfyun.cn/topic/info?type=medical-entity&ch=dc-web-35)
## ä¸­æ–‡æ•°æ®é›†

### 1. Yidu-S4Kï¼šåŒ»æ¸¡äº‘ç»“æ„åŒ–4Kæ•°æ®é›†

æ•°æ®é›†æè¿°ï¼š

> Yidu-S4K æ•°æ®é›†æºè‡ªCCKS 2019 è¯„æµ‹ä»»åŠ¡ä¸€ï¼Œå³â€œé¢å‘ä¸­æ–‡ç”µå­ç—…å†çš„å‘½åå®ä½“è¯†åˆ«â€çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå­ä»»åŠ¡ï¼š
> 1ï¼‰åŒ»ç–—å‘½åå®ä½“è¯†åˆ«ï¼šç”±äºå›½å†…æ²¡æœ‰å…¬å¼€å¯è·å¾—çš„é¢å‘ä¸­æ–‡ç”µå­ç—…å†åŒ»ç–—å®ä½“è¯†åˆ«æ•°æ®é›†ï¼Œæœ¬å¹´åº¦ä¿ç•™äº†åŒ»ç–—å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ï¼Œå¯¹2017å¹´åº¦æ•°æ®é›†åšäº†ä¿®è®¢ï¼Œå¹¶éšä»»åŠ¡ä¸€åŒå‘å¸ƒã€‚æœ¬å­ä»»åŠ¡çš„æ•°æ®é›†åŒ…æ‹¬è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
> 2ï¼‰åŒ»ç–—å®ä½“åŠå±æ€§æŠ½å–ï¼ˆè·¨é™¢è¿ç§»ï¼‰ï¼šåœ¨åŒ»ç–—å®ä½“è¯†åˆ«çš„åŸºç¡€ä¸Šï¼Œå¯¹é¢„å®šä¹‰å®ä½“å±æ€§è¿›è¡ŒæŠ½å–ã€‚æœ¬ä»»åŠ¡ä¸ºè¿ç§»å­¦ä¹ ä»»åŠ¡ï¼Œå³åœ¨åªæä¾›ç›®æ ‡åœºæ™¯å°‘é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å…¶ä»–åœºæ™¯çš„æ ‡æ³¨æ•°æ®åŠéæ ‡æ³¨æ•°æ®è¿›è¡Œç›®æ ‡åœºæ™¯çš„è¯†åˆ«ä»»åŠ¡ã€‚æœ¬å­ä»»åŠ¡çš„æ•°æ®é›†åŒ…æ‹¬è®­ç»ƒé›†ï¼ˆéç›®æ ‡åœºæ™¯å’Œç›®æ ‡åœºæ™¯çš„æ ‡æ³¨æ•°æ®ã€å„ä¸ªåœºæ™¯çš„éæ ‡æ³¨æ•°æ®ï¼‰å’Œæµ‹è¯•é›†ï¼ˆç›®æ ‡åœºæ™¯çš„æ ‡æ³¨æ•°æ®

[æ•°æ®é›†åœ°å€](http://openkg.cn/dataset/yidu-s4k)

åº¦ç›˜ä¸‹è½½åœ°å€ï¼šhttps://pan.baidu.com/s/1QqYtqDwhc_S51F3SYMChBQ

æå–ç ï¼šflql

### 2.ç‘é‡‘åŒ»é™¢ç³–å°¿ç—…æ•°æ®é›†

æ•°æ®é›†æè¿°ï¼š

>æ•°æ®é›†æ¥è‡ªå¤©æ± å¤§èµ›ã€‚æ­¤æ•°æ®é›†æ—¨åœ¨é€šè¿‡ç³–å°¿ç—…ç›¸å…³çš„æ•™ç§‘ä¹¦ã€ç ”ç©¶è®ºæ–‡æ¥åšç³–å°¿ç—…æ–‡çŒ®æŒ–æ˜å¹¶æ„å»ºç³–å°¿ç—…çŸ¥è¯†å›¾è°±ã€‚å‚èµ›é€‰æ‰‹éœ€è¦è®¾è®¡é«˜å‡†ç¡®ç‡ï¼Œé«˜æ•ˆçš„ç®—æ³•æ¥æŒ‘æˆ˜è¿™ä¸€ç§‘å­¦éš¾é¢˜ã€‚ç¬¬ä¸€èµ›å­£è¯¾é¢˜ä¸ºâ€œåŸºäºç³–å°¿ç—…ä¸´åºŠæŒ‡å—å’Œç ”ç©¶è®ºæ–‡çš„å®ä½“æ ‡æ³¨æ„å»ºâ€ï¼Œç¬¬äºŒèµ›å­£è¯¾é¢˜ä¸ºâ€œåŸºäºç³–å°¿ç—…ä¸´åºŠæŒ‡å—å’Œç ”ç©¶è®ºæ–‡çš„å®ä½“é—´å…³ç³»æ„å»ºâ€ã€‚

å®˜æ–¹æä¾›çš„æ•°æ®åªåŒ…å«è®­ç»ƒé›†ï¼ŒçœŸæ­£ç”¨äºæœ€ç»ˆæ’åçš„æµ‹è¯•é›†æ²¡æœ‰ç»™å‡ºã€‚

[æ•°æ®é›†åœ°å€](https://tianchi.aliyun.com/competition/entrance/231687/information)

åº¦ç›˜ä¸‹è½½åœ°å€ï¼šhttps://pan.baidu.com/s/1CWKblBNBqR-vs2h0xiXSdQ

æå–ç ï¼š0c54

### 3.Yidu-N7Kï¼šåŒ»æ¸¡äº‘æ ‡å‡†åŒ–7Kæ•°æ®é›†

æ•°æ®é›†æè¿°ï¼š

>Yidu-N4K æ•°æ®é›†æºè‡ªCHIP 2019 è¯„æµ‹ä»»åŠ¡ä¸€ï¼Œå³â€œä¸´åºŠæœ¯è¯­æ ‡å‡†åŒ–ä»»åŠ¡â€çš„æ•°æ®é›†ã€‚
>ä¸´åºŠæœ¯è¯­æ ‡å‡†åŒ–ä»»åŠ¡æ˜¯åŒ»å­¦ç»Ÿè®¡ä¸­ä¸å¯æˆ–ç¼ºçš„ä¸€é¡¹ä»»åŠ¡ã€‚ä¸´åºŠä¸Šï¼Œå…³äºåŒä¸€ç§è¯Šæ–­ã€æ‰‹æœ¯ã€è¯å“ã€æ£€æŸ¥ã€åŒ–éªŒã€ç—‡çŠ¶ç­‰å¾€å¾€ä¼šæœ‰æˆç™¾ä¸Šåƒç§ä¸åŒçš„å†™æ³•ã€‚æ ‡å‡†åŒ–ï¼ˆå½’ä¸€ï¼‰è¦è§£å†³çš„é—®é¢˜å°±æ˜¯ä¸ºä¸´åºŠä¸Šå„ç§ä¸åŒè¯´æ³•æ‰¾åˆ°å¯¹åº”çš„æ ‡å‡†è¯´æ³•ã€‚æœ‰äº†æœ¯è¯­æ ‡å‡†åŒ–çš„åŸºç¡€ï¼Œç ”ç©¶äººå‘˜æ‰å¯å¯¹ç”µå­ç—…å†è¿›è¡Œåç»­çš„ç»Ÿè®¡åˆ†æã€‚æœ¬è´¨ä¸Šï¼Œä¸´åºŠæœ¯è¯­æ ‡å‡†åŒ–ä»»åŠ¡ä¹Ÿæ˜¯è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ä»»åŠ¡çš„ä¸€ç§ã€‚ä½†æ˜¯ç”±äºåŸè¯è¡¨è¿°æ–¹å¼è¿‡äºå¤šæ ·ï¼Œå•ä¸€çš„åŒ¹é…æ¨¡å‹å¾ˆéš¾è·å¾—å¾ˆå¥½çš„æ•ˆæœã€‚

[æ•°æ®é›†åœ°å€](http://openkg.cn/dataset/yidu-n7k)

### 4.ä¸­æ–‡åŒ»å­¦é—®ç­”æ•°æ®é›†

æ•°æ®é›†æè¿°ï¼š

>ä¸­æ–‡åŒ»è¯æ–¹é¢çš„é—®ç­”æ•°æ®é›†ï¼Œè¶…è¿‡10ä¸‡æ¡ã€‚

æ•°æ®è¯´æ˜:

>questions.csvï¼šæ‰€æœ‰çš„é—®é¢˜åŠå…¶å†…å®¹ã€‚answers.csv ï¼šæ‰€æœ‰é—®é¢˜çš„ç­”æ¡ˆã€‚
>train_candidates.txtï¼Œ dev_candidates.txtï¼Œ test_candidates.txt ï¼šå°†ä¸Šè¿°ä¸¤ä¸ªæ–‡ä»¶è¿›è¡Œäº†æ‹†åˆ†ã€‚

[æ•°æ®é›†åœ°å€](https://www.kesci.com/home/dataset/5d313070cf76a60036e4b023/document)

[æ•°æ®é›†githubåœ°å€](https://github.com/zhangsheng93/cMedQA2)

### 5.å¹³å®‰åŒ»ç–—ç§‘æŠ€ç–¾ç—…é—®ç­”è¿ç§»å­¦ä¹ æ¯”èµ›

æ•°æ®é›†æè¿°ï¼š

>æœ¬æ¬¡æ¯”èµ›æ˜¯chip2019ä¸­çš„è¯„æµ‹ä»»åŠ¡äºŒï¼Œç”±å¹³å®‰åŒ»ç–—ç§‘æŠ€ä¸»åŠã€‚chip2019ä¼šè®®è¯¦æƒ…è§é“¾æ¥ï¼šhttp://cips-chip.org.cn/evaluation
>è¿ç§»å­¦ä¹ æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é‡è¦ä¸€ç¯ï¼Œå…¶ä¸»è¦ç›®çš„æ˜¯é€šè¿‡ä»å·²å­¦ä¹ çš„ç›¸å…³ä»»åŠ¡ä¸­è½¬ç§»çŸ¥è¯†æ¥æ”¹è¿›æ–°ä»»åŠ¡çš„å­¦ä¹ æ•ˆæœï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
>æœ¬æ¬¡è¯„æµ‹ä»»åŠ¡çš„ä¸»è¦ç›®æ ‡æ˜¯é’ˆå¯¹ä¸­æ–‡çš„ç–¾ç—…é—®ç­”æ•°æ®ï¼Œè¿›è¡Œç—…ç§é—´çš„è¿ç§»å­¦ä¹ ã€‚å…·ä½“è€Œè¨€ï¼Œç»™å®šæ¥è‡ª5ä¸ªä¸åŒç—…ç§çš„é—®å¥å¯¹ï¼Œè¦æ±‚åˆ¤å®šä¸¤ä¸ªå¥å­è¯­ä¹‰æ˜¯å¦ç›¸åŒæˆ–è€…ç›¸è¿‘ã€‚æ‰€æœ‰è¯­æ–™æ¥è‡ªäº’è”ç½‘ä¸Šæ‚£è€…çœŸå®çš„é—®é¢˜ï¼Œå¹¶ç»è¿‡äº†ç­›é€‰å’Œäººå·¥çš„æ„å›¾åŒ¹é…æ ‡æ³¨ã€‚

[æ•°æ®é›†åœ°å€(éœ€æ³¨å†Œ)](https://www.biendata.com/competition/chip2019/)

### 6.å¤©æ± æ–°å† è‚ºç‚é—®å¥åŒ¹é…æ¯”èµ›

æ•°æ®é›†æè¿°ï¼š

>æœ¬æ¬¡å¤§èµ›æ•°æ®åŒ…æ‹¬ï¼šè„±æ•ä¹‹åçš„åŒ»ç–—é—®é¢˜æ•°æ®å¯¹å’Œæ ‡æ³¨æ•°æ®ã€‚åŒ»ç–—é—®é¢˜æ¶‰åŠâ€œè‚ºç‚â€ã€â€œæ”¯åŸä½“è‚ºç‚â€ã€â€œæ”¯æ°”ç®¡ç‚â€ã€â€œä¸Šå‘¼å¸é“æ„ŸæŸ“â€ã€â€œè‚ºç»“æ ¸â€ã€â€œå“®å–˜â€ã€â€œèƒ¸è†œç‚â€ã€â€œè‚ºæ°”è‚¿â€ã€â€œæ„Ÿå†’â€ã€â€œå’³è¡€â€ç­‰10ä¸ªç—…ç§ã€‚
>æ•°æ®å…±åŒ…å«train.csvã€dev.csvã€test.csvä¸‰ä¸ªæ–‡ä»¶ï¼Œå…¶ä¸­ç»™å‚èµ›é€‰æ‰‹çš„æ–‡ä»¶åŒ…å«è®­ç»ƒé›†train.csvå’ŒéªŒè¯é›†dev.csvï¼Œæµ‹è¯•é›†test.csv å¯¹å‚èµ›é€‰æ‰‹ä¸å¯è§ã€‚
>æ¯ä¸€æ¡æ•°æ®ç”± Categoryï¼ŒQuery1ï¼ŒQuery2ï¼ŒLabelæ„æˆï¼Œåˆ†åˆ«è¡¨ç¤ºé—®é¢˜ç±»åˆ«ã€é—®å¥1ã€é—®å¥2ã€æ ‡ç­¾ã€‚Labelè¡¨ç¤ºé—®å¥ä¹‹é—´çš„è¯­ä¹‰æ˜¯å¦ç›¸åŒï¼Œè‹¥ç›¸åŒï¼Œæ ‡ä¸º1ï¼Œè‹¥ä¸ç›¸åŒï¼Œæ ‡ä¸º0ã€‚å…¶ä¸­ï¼Œè®­ç»ƒé›†Labelå·²çŸ¥ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†LabelæœªçŸ¥ã€‚
>ç¤ºä¾‹
>ç±»åˆ«ï¼šè‚ºç‚
>é—®å¥1ï¼šè‚ºéƒ¨å‘ç‚æ˜¯ä»€ä¹ˆåŸå› å¼•èµ·çš„ï¼Ÿ
>é—®å¥2ï¼šè‚ºéƒ¨å‘ç‚æ˜¯ä»€ä¹ˆå¼•èµ·çš„
>æ ‡ç­¾:1
>ç±»åˆ«ï¼šè‚ºç‚
>é—®å¥1ï¼šè‚ºéƒ¨å‘ç‚æ˜¯ä»€ä¹ˆåŸå› å¼•èµ·çš„ï¼Ÿ
>é—®å¥2ï¼šè‚ºéƒ¨ç‚ç—‡æœ‰ä»€ä¹ˆç—‡çŠ¶
>æ ‡ç­¾:0

[æ•°æ®é›†åœ°å€(éœ€æ³¨å†Œ)](https://tianchi.aliyun.com/competition/entrance/231776/information)

[çº¿ä¸Šç¬¬å››åè§£å†³æ–¹æ¡ˆåŠä»£ç ](https://github.com/Makaixin/similar-sentence-pairs-in-epidemic)

[çº¿ä¸Šç¬¬ä¸€åè§£å†³æ–¹æ¡ˆåŠä»£ç ](https://github.com/zzy99/epidemic-sentence-pair)

### 7.ä¸­æ–‡åŒ»æ‚£é—®ç­”å¯¹è¯æ•°æ®

æ•°æ®è¯´æ˜: æ¥è‡ªæŸåœ¨çº¿æ±‚åŒ»äº§å“çš„ä¸­æ–‡åŒ»æ‚£å¯¹è¯æ•°æ®ã€‚

åŸå§‹æè¿°:The MedDialog dataset contains conversations (in Chinese) between doctors and patients. It has 1.1 million dialogues and 4 million utterances. The data is continuously growing and more dialogues will be added. The raw dialogues are from haodf.com. All copyrights of the data belong to haodf.com.

[é¡¹ç›®åœ°å€](https://github.com/UCSD-AI4H/Medical-Dialogue-System)

åº¦ç›˜ä¸‹è½½åœ°å€: https://pan.baidu.com/s/1ZwzNgvAAMQk4klerTspsoA

æå–ç : lbo4

### 8.ä¸­æ–‡åŒ»å­¦é—®ç­”æ•°æ®

æ•°æ®è¯´æ˜: åŒ…å«å…­ä¸ªç§‘å®¤çš„åŒ»å­¦é—®ç­”æ•°æ®ï¼Œæ¥æºä¸æ˜ã€‚

[é¡¹ç›®åœ°å€](https://github.com/Toyhom/Chinese-medical-dialogue-data)

### 9.CHIP2020å„é¡¹è¯„æµ‹å·²å¼€æ”¾

CHIP2020å„é¡¹è¯„æµ‹å·²å¼€æ”¾ï¼ŒåŒ…æ‹¬åŒ»å­¦é¢†åŸŸçš„å®ä½“è¯†åˆ«ï¼Œå…³ç³»æŠ½å–ï¼Œæ–‡æœ¬ç”Ÿæˆï¼Œæœ¯è¯­æ ‡å‡†åŒ–ç­‰ä»»åŠ¡ï¼Œå¯ä»¥å‰å¾€å®˜ç½‘æŸ¥é˜…ã€‚

[CHIP2020å®˜æ–¹ç½‘å€](http://cips-chip.org.cn/2020/)

### 10.åŒ»å­¦æ•°æ®æŒ–æ˜ä¸ç®—æ³•è¯„æµ‹å¤§èµ›

æ–°é²œå‡ºç‚‰ï¼Œè¯¦æƒ…å‚çœ‹paperweeklyå…¬ä¼—å·ã€‚[æ–‡ç« é“¾æ¥](https://mp.weixin.qq.com/s/3hiJy8m0VohYfEH5ISta5w)

### 11.ä¸­æ–‡åŒ»ç–—å¯¹è¯æ•°æ®é›†

githubå¼€æºæ•°æ®

[é¡¹ç›®åœ°å€](https://github.com/Toyhom/Chinese-medical-dialogue-data)

### 12.é˜¿é‡Œå‘å¸ƒçš„ä¸­æ–‡åŒ»ç–—æ ‡å‡†æ•°æ®é›†åˆ

é˜¿é‡Œå›¢é˜Ÿå‘å¸ƒçš„ä¸­æ–‡åŒ»ç–—NLPç›¸å…³è¯„æµ‹æ•°æ®é›†åˆChinese_BLUE,å‘è¡¨äºWSDM2020ã€‚å¦å¤–:é¡¹ç›®ä¸­è¯´æ­¤é¡¹ç›®éé˜¿é‡Œçš„å®˜æ–¹äº§å“ï¼Œæ‰€ä»¥ä»…ä¾›å‚è€ƒã€‚

[é¡¹ç›®åœ°å€](https://github.com/alibaba-research/ChineseBLUE)

[è®ºæ–‡åœ°å€](https://arxiv.org/pdf/2008.10813.pdf)

## ä¸­æ–‡åŒ»å­¦çŸ¥è¯†å›¾è°±

### CMeKG

[åœ°å€](http://cmekg.pcl.ac.cn/)

ç®€ä»‹ï¼šCMeKGï¼ˆChinese Medical Knowledge Graphï¼‰æ˜¯åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸æ–‡æœ¬æŒ–æ˜æŠ€æœ¯ï¼ŒåŸºäºå¤§è§„æ¨¡åŒ»å­¦æ–‡æœ¬æ•°æ®ï¼Œä»¥äººæœºç»“åˆçš„æ–¹å¼ç ”å‘çš„ä¸­æ–‡åŒ»å­¦çŸ¥è¯†å›¾è°±ã€‚CMeKGçš„æ„å»ºå‚è€ƒäº†ICDã€ATCã€SNOMEDã€MeSHç­‰æƒå¨çš„å›½é™…åŒ»å­¦æ ‡å‡†ä»¥åŠè§„æ¨¡åºå¤§ã€å¤šæºå¼‚æ„çš„ä¸´åºŠæŒ‡å—ã€è¡Œä¸šæ ‡å‡†ã€è¯Šç–—è§„èŒƒä¸åŒ»å­¦ç™¾ç§‘ç­‰åŒ»å­¦æ–‡æœ¬ä¿¡æ¯ã€‚CMeKG 1.0åŒ…æ‹¬ï¼š6310ç§ç–¾ç—…ã€19853ç§è¯ç‰©ï¼ˆè¥¿è¯ã€ä¸­æˆè¯ã€ä¸­è‰è¯ï¼‰ã€1237ç§è¯Šç–—æŠ€æœ¯åŠè®¾å¤‡çš„ç»“æ„åŒ–çŸ¥è¯†æè¿°ï¼Œæ¶µç›–ç–¾ç—…çš„ä¸´åºŠç—‡çŠ¶ã€å‘ç—…éƒ¨ä½ã€è¯ç‰©æ²»ç–—ã€æ‰‹æœ¯æ²»ç–—ã€é‰´åˆ«è¯Šæ–­ã€å½±åƒå­¦æ£€æŸ¥ã€é«˜å±å› ç´ ã€ä¼ æ’­é€”å¾„ã€å¤šå‘ç¾¤ä½“ã€å°±è¯Šç§‘å®¤ç­‰ä»¥åŠè¯ç‰©çš„æˆåˆ†ã€é€‚åº”ç—‡ã€ç”¨æ³•ç”¨é‡ã€æœ‰æ•ˆæœŸã€ç¦å¿Œè¯ç­‰30ä½™ç§å¸¸è§å…³ç³»ç±»å‹ï¼ŒCMeKGæè¿°çš„æ¦‚å¿µå…³ç³»å®ä¾‹åŠå±æ€§ä¸‰å…ƒç»„è¾¾100ä½™ä¸‡ã€‚

## è‹±æ–‡æ•°æ®é›†

### PubMedQA: A Dataset for Biomedical Research Question Answering

æ•°æ®é›†æè¿°ï¼š åŸºäºPubmedæå–çš„åŒ»å­¦é—®ç­”æ•°æ®é›†ã€‚PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially gen- erated QA instances. 

[è®ºæ–‡åœ°å€](https://arxiv.org/abs/1909.06146)

### COMETA: A Corpus for Medical Entity Linking in the Social Media

æ•°æ®é›†æè¿°ï¼š ç¤¾äº¤åª’ä½“ä¸­çš„åŒ»ç–—å®ä½“é“¾æ¥æ•°æ®ã€‚å‘è¡¨äºEMNLP2020ã€‚

æ•°æ®è·å–æ–¹å¼:
COMETA is available by contacting the last author via e-mail or following the instructions on https://www.siphs.org/. We release the
pre-trained embeddings and the code to replicate our baselines online at https://github.com/cambridgeltl/cometa.

[è®ºæ–‡åœ°å€](https://arxiv.org/pdf/2010.03295.pdf)

### MedMentions

æ•°æ®é›†æè¿°ï¼š åŸºäºPubmedæ‘˜è¦çš„ç”Ÿç‰©åŒ»å­¦å®ä½“é“¾æ¥æ•°æ®é›†ã€‚å‘è¡¨äºAKBC 2019ã€‚

[æ•°æ®é›†åœ°å€](https://github.com/chanzuckerberg/MedMentions)

[è®ºæ–‡åœ°å€](https://arxiv.org/abs/1902.09476)

## ç›¸å…³è®ºæ–‡

### 1.åŒ»ç–—é¢†åŸŸé¢„è®­ç»ƒembedding

æ³¨ï¼šç›®å‰æ²¡æœ‰æ”¶é›†åˆ°ä¸­æ–‡åŒ»ç–—é¢†åŸŸçš„å¼€æºé¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥ä¸‹åˆ—å‡ºè‹±æ–‡è®ºæ–‡ä¾›å‚è€ƒã€‚

**Bio-bert**

è®ºæ–‡é¢˜ç›®ï¼šBioBERT: a pre-trained biomedical language representation model for biomedical text mining

[è®ºæ–‡åœ°å€](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btz682/5566506)

[é¡¹ç›®åœ°å€](https://github.com/dmis-lab/biobert)

è®ºæ–‡æ¦‚è¦ï¼šä»¥é€šç”¨é¢†åŸŸé¢„è®­ç»ƒbertä¸ºåˆå§‹æƒé‡ï¼ŒåŸºäºPubmedä¸Šå¤§é‡åŒ»ç–—é¢†åŸŸè‹±æ–‡è®ºæ–‡è®­ç»ƒã€‚åœ¨å¤šä¸ªåŒ»ç–—ç›¸å…³ä¸‹æ¸¸ä»»åŠ¡ä¸­è¶…è¶ŠSOTAæ¨¡å‹çš„è¡¨ç°ã€‚

è®ºæ–‡æ‘˜è¦ï¼š

> **Motivation**: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from bio- medical literature has gained popularity among researchers, and deep learning has boosted the development of ef- fective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text min- ing often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. 
> **Results**: We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. 
> **Availability and implementation**: We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.

**sci-bert**

è®ºæ–‡é¢˜ç›®ï¼šSCIBERT: A Pretrained Language Model for Scientific Text

[è®ºæ–‡åœ°å€](https://arxiv.org/abs/1903.10676)

[é¡¹ç›®åœ°å€](https://github.com/allenai/scibert/)

è®ºæ–‡æ¦‚è¦ï¼šAllenAI å›¢é˜Ÿå‡ºå“.åŸºäºSemantic Scholar ä¸Š 110ä¸‡+ æ–‡ç« è®­ç»ƒçš„ ç§‘å­¦é¢†åŸŸbert.

è®ºæ–‡æ‘˜è¦ï¼šObtaining large-scale annotated data for NLP tasks in the scientific domain is challeng- ing and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve perfor- mance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demon- strate statistically significant improvements over BERT and achieve new state-of-the- art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.

**clinical-bert**

è®ºæ–‡é¢˜ç›®ï¼šPublicly Available Clinical BERT Embeddings

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/W19-1909/)

[é¡¹ç›®åœ°å€](https://github.com/EmilyAlsentzer/clinicalBERT)

è®ºæ–‡æ¦‚è¦ï¼šå‡ºè‡ªNAACL Clinical NLP Workshop 2019.åŸºäºMIMIC-IIIæ•°æ®åº“ä¸­çš„200ä¸‡ä»½åŒ»ç–—è®°å½•è®­ç»ƒçš„ä¸´åºŠé¢†åŸŸbert.

è®ºæ–‡æ‘˜è¦ï¼šContextual word embedding models such as ELMo and BERT have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on 3/5 clinical NLP tasks, establishing a new state-of-the-art on the MedNLI dataset. We find that these domain-specific models are not as performant on 2 clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.

**clinical-bert(å¦ä¸€å›¢é˜Ÿçš„ç‰ˆæœ¬)**

è®ºæ–‡é¢˜ç›®ï¼šClinicalBert: Modeling Clinical Notes and Predicting Hospital Readmission

[è®ºæ–‡åœ°å€](https://arxiv.org/abs/1904.05342)

[é¡¹ç›®åœ°å€](https://github.com/kexinhuang12345/clinicalBERT)

è®ºæ–‡æ¦‚è¦ï¼šåŒæ ·åŸºäºMIMIC-IIIæ•°æ®åº“,ä½†åªéšæœºé€‰å–äº†10ä¸‡ä»½åŒ»ç–—è®°å½•è®­ç»ƒçš„ä¸´åºŠé¢†åŸŸbert.

è®ºæ–‡æ‘˜è¦ï¼šClinical notes contain information about patients that goes beyond structured data like lab values and medications. However, clinical notes have been underused relative to structured data, because notes are high-dimensional and sparse. This work develops and evaluates representations of clinical notes using bidirectional transformers (ClinicalBert). Clini- calBert uncovers high-quality relationships between medical concepts as judged by hu- mans. ClinicalBert outperforms baselines on 30-day hospital readmission prediction using both discharge summaries and the first few days of notes in the intensive care unit. Code and model parameters are available.

**BEHRT**

è®ºæ–‡é¢˜ç›®ï¼šBEHRT: TRANSFORMER FOR ELECTRONIC HEALTH RECORDS

[è®ºæ–‡åœ°å€](https://arxiv.org/abs/1907.09538)

é¡¹ç›®åœ°å€: æš‚æœªå¼€æº

è®ºæ–‡æ¦‚è¦ï¼šè¿™ç¯‡è®ºæ–‡ä¸­embeddingæ˜¯åŸºäºåŒ»å­¦å®ä½“è®­ç»ƒï¼Œè€Œä¸æ˜¯åŸºäºå•è¯ã€‚

è®ºæ–‡æ‘˜è¦ï¼šToday, despite decades of developments in medicine and the growing interest in precision healthcare, vast majority of diagnoses happen once patients begin to show noticeable signs of illness. Early indication and detection of diseases, however, can provide patients and carers with the chance of early intervention, better disease management, and efficient allocation of healthcare resources. The latest developments in machine learning (more specifically, deep learning) provides a great opportunity to address this unmet need. In this study, we introduce BEHRT: A deep neural sequence transduction model for EHR (electronic health records), capable of multitask prediction and disease trajectory mapping. When trained and evaluated on the data from nearly 1.6 million individuals, BEHRT shows a striking absolute improvement of 8.0-10.8%, in terms of Average Precision Score, compared to the existing state-of-the-art deep EHR models (in terms of average precision, when predicting for the onset of 301 conditions). In addition to its superior prediction power, BEHRT provides a personalised view of disease trajectories through its attention mechanism; its flexible architecture enables it to incorporate multiple heterogeneous concepts (e.g., diagnosis, medication, measurements, and more) to improve the accuracy of its predictions; and its (pre-)training results in disease and patient representations that can help us get a step closer to interpretable predictions.

### 2.ç»¼è¿°ç±»æ–‡ç« 

**nature medicineå‘è¡¨çš„ç»¼è¿°**

è®ºæ–‡é¢˜ç›®ï¼šA guide to deep learning in healthcare

[è®ºæ–‡åœ°å€](https://www.nature.com/articles/s41591-018-0316-z)

è®ºæ–‡æ¦‚è¦ï¼šå‘è¡¨äºnature medicineï¼ŒåŒ…å«åŒ»å­¦é¢†åŸŸä¸‹CV,NLP,å¼ºåŒ–å­¦ä¹ ç­‰æ–¹é¢çš„åº”ç”¨ç»¼è¿°ã€‚

è®ºæ–‡æ‘˜è¦ï¼šHere we present deep-learning techniques for healthcare, centering our discussion on deep learning in computer vision, natural language processing, reinforcement learning, and generalized methods. We describe how these computational techniques can impact a few key areas of medicine and explore how to build end-to-end systems. Our discussion of computer vision focuses largely on medical imaging, and we describe the application of natural language processing to domains such as electronic health record data. Similarly, reinforcement learning is discussed in the context of robotic-assisted surgery, and generalized deep- learning methods for genomics are reviewed.

### 3.ç”µå­ç—…å†ç›¸å…³æ–‡ç« 

**Transfer Learning from Medical Literature for Section Prediction in Electronic Health Records**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/D19-1492/)

è®ºæ–‡æ¦‚è¦ï¼šå‘è¡¨äºEMNLP2019ã€‚åŸºäºå°‘é‡in-domainæ•°æ®å’Œå¤§é‡out-of-domainæ•°æ®è¿›è¡ŒEHRç›¸å…³çš„è¿ç§»å­¦ä¹ ã€‚

è®ºæ–‡æ‘˜è¦ï¼šsections such as Assessment and Plan, So- cial History, and Medications. These sec- tions help physicians find information easily and can be used by an information retrieval system to return specific information sought by a user. However, it is common that the exact format of sections in a particular EHR does not adhere to known patterns. There- fore, being able to predict sections and headers in EHRs automatically is beneficial to physi- cians. Prior approaches in EHR section pre- diction have only used text data from EHRs and have required significant manual annota- tion. We propose using sections from med- ical literature (e.g., textbooks, journals, web content) that contain content similar to that found in EHR sections. Our approach uses data from a different kind of source where la- bels are provided without the need of a time- consuming annotation effort. We use this data to train two models: an RNN and a BERT- based model. We apply the learned models along with source data via transfer learning to predict sections in EHRs. Our results show that medical literature can provide helpful su- pervision signal for this classification task.

**MUFASA: Multimodal Fusion Architecture Search for Electronic Health Records**

[è®ºæ–‡åœ°å€](http://arxiv-download.xixiaoyao.cn/pdf/2102.02340.pdf)

è®ºæ–‡æ¦‚è¦:å‘è¡¨äºAAAI2021ã€‚

è®ºæ–‡æ‘˜è¦:One important challenge of applying deep learning to electronic health records (EHR) is the complexity of their multimodal structure. EHR usually contains a mixture of structured (codes) and unstructured (free-text) data with sparse and irregular longitudinal features â€“ all of which doctors utilize when making decisions. In the deep learning regime, determining how different modality representations should be fused together is a difficult problem, which is often addressed by handcrafted modeling and intuition. In this work, we extend state-of-the-art neural architecture search (NAS) methods and propose MUltimodal Fusion Architecture SeArch (MUFASA) to simultaneously search across multimodal fusion strategies and modality-specific architectures for the first time. We demonstrate empirically that our MUFASA method outperforms established unimodal NAS on public EHR data with comparable computation costs. In addition, MUFASA produces architectures that outperform Transformer and Evolved Transformer. Compared with these baselines on CCS diagnosis code prediction, our discovered models improve top-5 recall from 0.88 to 0.91 and demonstrate the ability to generalize to other EHR tasks. Studying our top architecture in depth, we provide empirical evidence that MUFASAâ€™s improvements are derived from its ability to both customize modeling for each data modality and find effective fusion strategies.


### 4.åŒ»å­¦å…³ç³»æŠ½å–

**Leveraging Dependency Forest for Neural Medical Relation Extraction**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/D19-1020/)

è®ºæ–‡æ¦‚è¦ï¼šå‘è¡¨äºEMNLP 2019. åŸºäºdependency forestæ–¹æ³•ï¼Œæå‡å¯¹åŒ»å­¦è¯­å¥ä¸­ä¾å­˜å…³ç³»çš„å¬å›ç‡ï¼ŒåŒæ—¶å¼•è¿›äº†ä¸€éƒ¨åˆ†å™ªå£°ï¼ŒåŸºäºå›¾å¾ªç¯ç½‘ç»œè¿›è¡Œç‰¹å¾æå–ï¼Œæä¾›äº†åœ¨åŒ»ç–—å…³ç³»æŠ½å–ä¸­ä½¿ç”¨ä¾å­˜å…³ç³»ï¼ŒåŒæ—¶å‡å°‘è¯¯å·®ä¼ é€’çš„ä¸€ç§æ€è·¯ã€‚

è®ºæ–‡æ‘˜è¦ï¼šMedical relation extraction discovers relations between entity mentions in text, such as research articles. For this task, dependency syntax has been recognized as a crucial source of features. Yet in the medical domain, 1-best parse trees suffer from relatively low accuracies, diminishing their usefulness. We investigate a method to alleviate this problem by utilizing dependency forests. Forests contain more than one possible decisions and therefore have higher recall but more noise compared with 1-best outputs. A graph neural network is used to represent the forests, automatically distinguishing the useful syntactic information from parsing noise. Results on two benchmarks show that our method outperforms the standard tree-based methods, giving the state-of-the-art results in the literature.

### 5.åŒ»å­¦çŸ¥è¯†å›¾è°±

**Learning a Health Knowledge Graph from Electronic Medical Records**

[è®ºæ–‡åœ°å€](https://www.nature.com/articles/s41598-017-05778-z)

è®ºæ–‡æ¦‚è¦ï¼šå‘è¡¨äºnature scientificreportsï¼ˆ2017ï¼‰. åŸºäº27ä¸‡ä½™ä»½ç”µå­ç—…å†æ„å»ºçš„ç–¾ç—…-ç—‡çŠ¶çŸ¥è¯†å›¾è°±ã€‚

è®ºæ–‡æ‘˜è¦ï¼šDemand for clinical decision support systems in medicine and self-diagnostic symptom checkers has substantially increased in recent years. Existing platforms rely on knowledge bases manually compiled through a labor-intensive process or automatically derived using simple pairwise statistics. This study explored an automated process to learn high quality knowledge bases linking diseases and symptoms directly from electronic medical records. Medical concepts were extracted from 273,174 de-identified patient records and maximum likelihood estimation of three probabilistic models was used to automatically construct knowledge graphs: logistic regression, naive Bayes classifier and a Bayesian network using noisy OR gates. A graph of disease-symptom relationships was elicited from the learned parameters and the constructed knowledge graphs were evaluated and validated, with permission, against Googleâ€™s manually-constructed knowledge graph and against expert physician opinions. Our study shows that direct and automated construction of high quality health knowledge graphs from medical records using rudimentary concept extraction is feasible. The noisy OR model produces a high quality knowledge graph reaching precision of 0.85 for a recall of 0.6 in the clinical evaluation. Noisy OR significantly outperforms all tested models across evaluation frameworks (pâ€‰<â€‰0.01).

### 6.è¾…åŠ©è¯Šæ–­

**Evaluation and accurate diagnoses of pediatric diseases using artificial intelligence**

[è®ºæ–‡åœ°å€](https://www.nature.com/articles/s41591-018-0335-9)

è®ºæ–‡æ¦‚è¦ï¼šè¯¥æ–‡ç« ç”±å¹¿å·å¸‚å¦‡å¥³å„¿ç«¥åŒ»ç–—ä¸­å¿ƒä¸ä¾å›¾åŒ»ç–—ç­‰ä¼ä¸šå’Œç§‘ç ”æœºæ„å…±åŒå®Œæˆï¼ŒåŸºäºæœºå™¨å­¦ä¹ çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯å®ç°ä¸è¾“äººç±»åŒ»ç”Ÿçš„å¼ºå¤§è¯Šæ–­èƒ½åŠ›ï¼Œå¹¶å…·å¤‡å¤šåœºæ™¯çš„åº”ç”¨èƒ½åŠ›ã€‚æ®ä»‹ç»ï¼Œè¿™æ˜¯å…¨çƒé¦–æ¬¡åœ¨é¡¶çº§åŒ»å­¦æ‚å¿—å‘è¡¨æœ‰å…³è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯åŸºäºç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰åšä¸´åºŠæ™ºèƒ½è¯Šæ–­çš„ç ”ç©¶æˆæœï¼Œä¹Ÿæ˜¯åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯è¯Šæ–­å„¿ç§‘ç–¾ç—…çš„é‡ç£…ç§‘ç ”æˆæœã€‚

è®ºæ–‡æ‘˜è¦ï¼šArtificial intelligence (AI)-based methods have emerged as powerful tools to transform medical care. Although machine learning classifiers (MLCs) have already demonstrated strong performance in image-based diagnoses, analysis of diverse and massive electronic health record (EHR) data remains challenging. Here, we show that MLCs can query EHRs in a manner similar to the hypothetico-deductive reasoning used by physicians and unearth associations that previous statistical methods have not found. Our model applies an automated natural language processing system using deep learning techniques to extract clinically relevant information from EHRs. In total, 101.6 million data points from 1,362,559 pediatric patient visits presenting to a major referral center were analyzed to train and validate the framework. Our model demonstrates high diagnostic accuracy across multiple organ systems and is comparable to experienced pediatricians in diagnosing common childhood diseases. Our study provides a proof of concept for implementing an AI-based system as a means to aid physicians in tackling large amounts of data, augmenting diagnostic evaluations, and to provide clinical decision support in cases of diagnostic uncertainty or complexity. Although this impact may be most evident in areas where healthcare providers are in relative shortage, the benefits of such an AI system are likely to be universal.

### 7.ACL2020åŒ»å­¦é¢†åŸŸç›¸å…³è®ºæ–‡åˆ—è¡¨

**A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/2020.acl-main.748/)

**Biomedical Entity Representations with Synonym Marginalization**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/2020.acl-main.335/)

**Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/2020.acl-main.613/)

**MIE: A Medical Information Extractor towards Medical Dialogues**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/2020.acl-main.576/)

**Rationalizing Medical Relation Prediction from Corpus-level Statistics**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/2020.acl-main.719/)

### 8.åŒ»ç–—å®ä½“Linkingï¼ˆæ ‡å‡†åŒ–ï¼‰

**Medical Entity Linking using Triplet Network**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/W19-1912/)

è®ºæ–‡æ¦‚è¦ï¼š å‘è¡¨äºACL2019,è®ºæ–‡å†…å®¹ä¸ºç–¾ç—…å®ä½“Linkingç ”ç©¶ã€‚ä½¿ç”¨ä¸‰å…ƒç»„æ•°æ®ï¼Œï¼ˆmentionï¼Œæ­£ä¾‹ï¼Œè´Ÿä¾‹ï¼‰ï¼Œç›®æ ‡ä½¿distance(mention,è´Ÿä¾‹)-distance(mention,æ­£ä¾‹)>alphaï¼ˆäººè„¸è¯†åˆ«çš„ç»å…¸æ–¹æ¡ˆï¼‰,å…·ä½“æŸå¤±å‡½æ•°å‚çœ‹è®ºæ–‡ã€‚è®ºæ–‡ä¸»è¦åŒ…æ‹¬ä¸¤éƒ¨åˆ†å†…å®¹1ï¼‰å€™é€‰æ•°æ®é›†ç”Ÿæˆ,å¯¹ç»™å®šmentionï¼Œä¸æ ‡å‡†ç–¾ç—…é›†åˆæ•°æ®ï¼ˆæ ‡å‡†è¯åŠåŒä¹‰è¯ï¼‰è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦åŠJaccard overlapåˆ†æ•°,å–topKä½œä¸ºå€™é€‰æ ·ä¾‹ã€‚ 2ï¼‰ç½‘ç»œç»“æ„åŸºäºTriplet Networkã€‚è¯¦è§è®ºæ–‡ã€‚

**A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/2020.acl-main.748.pdf)

è®ºæ–‡æ¦‚è¦: å‘è¡¨äºACL2020ã€‚åŸºäºlist-wiseæ’åºå­¦ä¹ æ–¹æ³•ã€‚ä¸»è¦åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šåç»­æ•°æ®é›†ç”Ÿæˆ å’Œ  åŸºäºBERTçš„list-wiseæ’åºã€‚è¾ƒæ–°é¢–çš„æ€è·¯ï¼š1ï¼‰åœ¨æ ·æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå¯¹æ ‡å‡†è¯è¿›è¡Œäº†åŸºäºåŒä¹‰è¯çš„æ‰©å±•ã€‚2ï¼‰åœ¨lossä¸­å¼•å…¥äº†è¯­ä¹‰ç±»å‹æ­£åˆ™åŒ–ã€‚è¯¦è§è®ºæ–‡ã€‚

**Deep Neural Models for Medical Concept Normalization in User-Generated Texts**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/P19-2055.pdf)


### 9. AAAI2020 åŒ»å­¦NLPç›¸å…³è®ºæ–‡åˆ—è¡¨

**On the Generation of Medical Question-Answer Pairs**

[è®ºæ–‡åœ°å€](https://arxiv.org/pdf/1811.00681.pdf)

**LATTE: Latent Type Modeling for Biomedical Entity Linking**

[è®ºæ–‡åœ°å€](https://arxiv.org/pdf/1911.09787.pdf)

**Learning Conceptual-Contextual Embeddings for Medical Text**

[è®ºæ–‡åœ°å€](https://arxiv.org/pdf/1908.06203.pdf)

**Understanding Medical Conversations with Scattered Keyword Attention and Weak Supervision from Responses**

[è®ºæ–‡åœ°å€](http://ir.hit.edu.cn/~car/papers/AAAI2020-Shi-medconv.pdf)

**Simultaneously Linking Entities and Extracting Relations from Biomedical Text without Mention-level Supervision**

[è®ºæ–‡åœ°å€](https://arxiv.org/pdf/1912.01070.pdf)

**Can Embeddings Adequately Represent Medical Terminology? New Large-Scale Medical Term Similarity Datasets Have the Answer!**

[è®ºæ–‡åœ°å€](https://arxiv.org/pdf/2003.11082.pdf)

### 10. EMNLP2020 åŒ»å­¦NLPç›¸å…³è®ºæ–‡åˆ—è¡¨

**Towards Medical Machine Reading Comprehension with Structural Knowledge and Plain Text**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/2020.emnlp-main.111.pdf)

**MedDialog: Large-scale Medical Dialogue Datasets**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/2020.emnlp-main.743.pdf)

**COMETA: A Corpus for Medical Entity Linking in the Social Media**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/2020.emnlp-main.253.pdf)

**Biomedical Event Extraction as Sequence Labeling**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/2020.emnlp-main.431.pdf)

**FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/2020.emnlp-main.165.pdf)

[è®ºæ–‡è§£æ:FedED:ç”¨äºåŒ»å­¦å…³ç³»æå–çš„è”é‚¦å­¦ä¹ (åŸºäºèåˆè’¸é¦)](https://blog.csdn.net/lrs1353281004/article/details/111877017)

**Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition**

[è®ºæ–‡åœ°å€](https://arxiv.org/pdf/2010.03746.pdf)

**A Knowledge-driven Generative Model for Multi-implication Chinese Medical Procedure Entity Normalization**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/2020.emnlp-main.116.pdf)

**BioMegatron: Larger Biomedical Domain Language Model**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/2020.emnlp-main.379.pdf)

**Querying Across Genres for Medical Claims in News**

[è®ºæ–‡åœ°å€](https://www.aclweb.org/anthology/2020.emnlp-main.139.pdf)


## ä¸­æ–‡åŒ»ç–—é¢†åŸŸè¯­æ–™

### åŒ»å­¦æ•™æ+åŸ¹è®­è€ƒè¯• 

è¯´æ˜:ç”±äºç‰ˆæƒåŸå› ï¼Œç°åœ¨æ— æ³•æä¾›åº¦ç›˜ä¸‹è½½é“¾æ¥äº†ï¼Œè¯·å¤§å®¶å‰å¾€åŸè±†ç“£é“¾æ¥ä¸‹è½½å§ã€‚

è¯­æ–™è¯´æ˜ï¼šæ ¹æ®æ­¤[è±†ç“£é“¾æ¥](https://www.douban.com/note/692003915/)æ•´ç†ã€‚

æ•°æ®é¢„è§ˆï¼š

![image](https://github.com/lrs1353281004/Chinese_medical_NLP/blob/master/images/pretrain_overview.png)

### å“ˆå·¥å¤§ã€Šå¤§è¯æ—ã€‹å¼€æ”¾75ä¸‡æ ¸å¿ƒå®ä½“è¯åŠç›¸å…³æ¦‚å¿µã€å…³ç³»åˆ—è¡¨ï¼ˆåŒ…å«ä¸­è¯/åŒ»é™¢/ç”Ÿç‰© ç±»åˆ«ï¼‰

è¯­æ–™è¯´æ˜:å“ˆå·¥å¤§å¼€æºäº†ã€Šå¤§è¯æ—ã€‹ä¸­çš„75ä¸‡çš„æ ¸å¿ƒå®ä½“è¯ï¼Œä»¥åŠè¿™äº›æ ¸å¿ƒå®ä½“è¯å¯¹åº”çš„ç»†ç²’åº¦æ¦‚å¿µè¯ï¼ˆå…±1.8ä¸‡æ¦‚å¿µè¯ï¼Œ300ä¸‡å®ä½“-æ¦‚å¿µå…ƒç»„ï¼‰ï¼Œè¿˜æœ‰ç›¸å…³çš„å…³ç³»ä¸‰å…ƒç»„ï¼ˆå…±300ä¸‡ï¼‰ã€‚è¿™75ä¸‡æ ¸å¿ƒå®ä½“åˆ—è¡¨æ¶µç›–äº†å¸¸è§çš„äººåã€åœ°åã€ç‰©å“åç­‰æœ¯è¯­ã€‚æ¦‚å¿µè¯åˆ—è¡¨åˆ™åŒ…å«äº†ç»†ç²’åº¦çš„å®ä½“æ¦‚å¿µä¿¡æ¯ã€‚å€ŸåŠ©äºç»†ç²’åº¦çš„ä¸Šä½æ¦‚å¿µå±‚æ¬¡ç»“æ„å’Œä¸°å¯Œçš„å®ä½“é—´å…³ç³»ï¼Œæœ¬æ¬¡å¼€æºçš„æ•°æ®èƒ½å¤Ÿä¸ºäººæœºå¯¹è¯ã€æ™ºèƒ½æ¨èã€ç­‰åº”ç”¨æŠ€æœ¯æä¾›æ•°æ®æ”¯æŒã€‚

[è¯­æ–™å®˜æ–¹ä¸‹è½½åœ°å€](http://101.200.120.155/browser/)

è¯´æ˜: é€šè¿‡ç½‘ä¸ŠæŸ¥è¯¢ï¼Œè¿™éƒ¨åˆ†èµ„æºåº”è¯¥æ˜¯è¢«ä¸€äº›å…¬å¸ä»˜è´¹ä½¿ç”¨äº†ï¼Œå¯èƒ½æœ‰ç‰ˆæƒé—®é¢˜ï¼Œæ‰€ä»¥ç°åœ¨ä¸‹è½½é“¾æ¥éƒ½å¤±æ•ˆäº†ã€‚åç»­å¦‚æœå†æœ‰å¼€æºçš„ä¿¡æ¯å†è¿›è¡Œæ›´æ–°ã€‚

## åŒ»å­¦embeddingåŠé¢„è®­ç»ƒæ¨¡å‹

### å¼€æºè‹±æ–‡åŒ»å­¦embedding

é¡¹ç›®è¯´æ˜ï¼šå‘è¡¨äºAMIA 2016. å¼€æºåŒ»å­¦ç›¸å…³æ¦‚å¿µembedding. 

[é¡¹ç›®åœ°å€](https://github.com/clinicalml/embeddings)

### MC-BERTä¸­æ–‡åŒ»ç–—é¢„è®­ç»ƒæ¨¡å‹

è®ºæ–‡åç§°ï¼šConceptualized Representation Learning for Chinese Biomedical Text Mining

[é¡¹ç›®åœ°å€](https://github.com/alibaba-research/ChineseBLUE)

[è§£è¯»æ–‡ç« ](https://zhuanlan.zhihu.com/p/208721884)

## å¼€æºå·¥å…·åŒ…

### åˆ†è¯å·¥å…·

#### PKUSEG

[é¡¹ç›®åœ°å€](https://github.com/lancopku/pkuseg-python)

é¡¹ç›®è¯´æ˜ï¼š åŒ—äº¬å¤§å­¦æ¨å‡ºçš„å¤šé¢†åŸŸä¸­æ–‡åˆ†è¯å·¥å…·ï¼Œæ”¯æŒé€‰æ‹©åŒ»å­¦é¢†åŸŸã€‚

## å·¥ä¸šçº§äº§å“è§£å†³æ–¹æ¡ˆ

[çµåŒ»æ™ºæ…§](https://01.baidu.com/index.html)

[å·¦æ‰‹åŒ»ç”Ÿ](https://open.zuoshouyisheng.com/)

[åŒ»æ¸¡äº‘ç ”ç©¶é™¢-åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†](https://www.yiducloud.com.cn/academy.html)

[ç™¾åº¦-åŒ»å­¦æ–‡æœ¬ç»“æ„åŒ–](https://ai.baidu.com/solution/mtp)

[é˜¿é‡Œäº‘-åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†](https://help.aliyun.com/document_detail/179395.html)

## blogåˆ†äº«

[åŒ»ç–—é¢†åŸŸæ„å»ºè‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿçš„ç»éªŒæ•™è®­](http://www.oreilly.com.cn/radar/?p=2083)

[å¤§æ•°æ®æ—¶ä»£çš„åŒ»å­¦å…¬å…±æ•°æ®åº“ä¸æ•°æ®æŒ–æ˜æŠ€æœ¯ç®€ä»‹](https://mp.weixin.qq.com/s/tA44U4bJUttnROfrzpNYcQ)

[ä»ACL 2021ä¸­çœ‹NLPåœ¨åŒ»ç–—é¢†åŸŸåº”ç”¨çš„å‘å±•ï¼Œé™„èµ„æºä¸‹è½½](https://mp.weixin.qq.com/s/RhcHvRWHRnYUg6u9vXoIGA)

## å‹æƒ…é“¾æ¥

[awesome_Chinese_medical_NLP](https://github.com/GanjinZero/awesome_Chinese_medical_NLP)

[ä¸­æ–‡NLPæ•°æ®é›†æœç´¢](https://www.cluebenchmarks.com/dataSet_search.html)

[medical-data(æµ·é‡åŒ»ç–—ç›¸å…³æ•°æ®)](https://github.com/beamandrew/medical-data)

[å¤©æ± æ•°æ®é›†(å…¶ä¸­åŒ…å«å¤šä¸ªåŒ»ç–—NLPæ•°æ®é›†)](https://tianchi.aliyun.com/dataset)

